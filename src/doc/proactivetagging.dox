/**
*
* \defgroup icubclient_tagging Run the proactive tagging demo
* \brief This provides a brief tutorial on how to run the proactive tagging demo described in the paper.
To run the proactive tagging demo, first install all modules as described in the README. Then, adapt the `proactiveTagging.xml` within `icub-client/app/demos` to match your system setup (modify the node names). For the role of the individual components, we refer to our paper.

Once you start all modules, the drive for knowledge acquisition will decay (see HomeostaticModule and AllostaticController::updateAllostatic()). Once it hits the lower threshold, the iCub will first try to recognize the partner it is interacting with using the face recognition ability of SAM (see SAM.SAM_Drivers.SAMDriver_interaction.SAMDriver_interaction). If the face is not being recognized, it will ask the partner for the name (see proactiveTagging::exploreUnknownEntity() and proactiveTagging::getNameFromSAM()). The next times the threshold is hit, the iCub will either ask for the name of an object by pointing to it, or ask for the name of the body part by moving it. You can use speech input to provide the name of the object / body part. The possible list of (object, bodypart and agent) names is coded within the XML files in the `icub-client/app/proactiveTagging/conf` directory. Also, if the name of a body part is known but its corresponding tactile patch is not known yet, the iCub will move the body part and ask you to touch the corresponding part (see proactiveTagging::exploreTactileEntityWithName()).

Alternatively, you can give commands to the iCub. The speech commands are recognized within ears::updateModule() that calls the proper module according to the given command. You can say: "Please take the X" (pulling an object), "Give me the X" (pushing an object), "Look at the X", "Point the X", where X is the name of an object. The names are coded in the `icub-client/app/ears/conf/MainGrammar.xml` file. We recommend keeping these object names consistent with the names in `icub-client/app/proactiveTagging/conf`. If the object is known to the iCub, the iCub will straight away follow the command (see FollowingOrder::handleAction()). If the object is not known (see FollowingOrder::handleSearch()), the proactiveTagging::searchingEntity() method is called which first checks whether there are any unknown objects in the scene, and if so, how many of them. If there is only one unknown object, the robot considers that to be the one the human referred to. If there are multiple unknown objects, the iCub will instead try to infer which object is known by recognising a pointing action of the partner. Once the partner has pointed to the object, the iCub knows the name and can thus execute the requested action. One can also ask the iCub to move a body part (see FollowingOrder::handleActionBP()), or recognize an action (see RecognitionOrder::run() and SAM.SAM_Drivers.SAMDriver_ARWin.SAMDriver_ARWin)

* (This page can be edited at src/doc/proactivetagging.dox)
*/
